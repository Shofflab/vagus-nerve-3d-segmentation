## Overview

This repository contains the scripts that support the paper:

> *Automated 3D Segmentation of Human Vagus Nerve Fascicles and Epineurium from Micro-Computed Tomography Images Using Anatomy-Aware Neural Networks* (Zhang et al., 2025; https://doi.org/10.1101/2025.06.12.659370). 

- `nnunet_3d/` – custom losses, trainers, configs, and helper scripts that extend nnUNetv2.
- `src/metrics/` – Python scripts to calculate metrics.
- `src/plotting/` – R code that performs statistical analyses and makes figures/tables.
- `results/` – CSV files containing the metrics.
- `figures/` and `tables/` – figures and tables generated by the plotting scripts.

## Requirements

### Python (via mamba + pip)
- Install mamba by following the official instructions: https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html.
- We run everything from a mamba-managed Python 3.11 environment. Create one with:
  ```bash
  mamba create -n <ENV_NAME> python=3.11
  mamba activate <ENV_NAME>
  ```
- With the environment active, install packages from `requirements.txt`:
  ```bash
  pip install -r requirements.txt
  ```

### R
- Install R (version 4.2 or later) via the CRAN instructions: https://cran.r-project.org/.
- Add the plotting dependencies in one go with `install.packages(c("broom","ggplot2","ggpmisc","ggpubr","ggthemes","gt","gtsummary","here","janitor","patchwork","readr","rmarkdown","rstatix","scales","tidyverse"))`.

## Setup

### 1. Install nnUNet and dependencies

```bash
git clone https://github.com/MIC-DKFZ/nnUNet.git
cd nnUNet
pip install -e .
git checkout 750718fadb243438e41f24fb136127fa41ac675f  # v2.4.1-34-g750718f
```

### 2. Copy the custom overrides

Copy these files from this repository into your `nnunetv2` checkout:

| Source file                                      | Destination path                                                        |
| ------------------------------------------------ | ----------------------------------------------------------------------- |
| `nnunet_3d/compound_losses.py`                   | `nnunetv2/training/loss/compound_losses.py`                             |
| `nnunet_3d/nnUNetTrainer.py`                     | `nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py`                      |
| `nnunet_3d/nnUNetTrainerV2_Loss_DiceCETopo.py`   | `nnunetv2/training/nnUNetTrainer/nnUNetTrainerV2_Loss_DiceCETopo.py`    |
| `nnunet_3d/default_normalization_schemes.py`     | `nnunetv2/preprocessing/normalization/default_normalization_schemes.py` |
| `nnunet_3d/map_channel_name_to_normalization.py` | `nnunetv2/preprocessing/normalization/map_channel_name_to_normalization.py` |

## Workflow

### Dataset preparation
- To replicate the paper, download `code/Dataset012_VN3Dv2.zip` from the SPARC portal and extract it into your nnUNet raw data directory.
- Custom datasets should follow the [nnUNet dataset format](https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/dataset_format.md).

### Planning and preprocessing

```bash
nnUNetv2_plan_and_preprocess DATASET_ID 3d_fullres
```

Optional: copy `nnunet_3d/configs/splits_final.json` into `$nnUNet_preprocessed/<DATASET_NAME>/` to reuse the subject-level folds from the paper.

### Training

```bash
nnUNetv2_train DATASET_ID 3d_fullres FOLD -tr nnUNetTrainerV2_Loss_DiceCETopo -p nnUNetPlans
```

Run once per fold (0–4) to complete cross-validation.

### Inference
- nnUNet saves validation/test predictions automatically after each fold completes; refer to the [nnUNet inference docs](https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunetv2/inference/readme.md) for more options.
- For ad-hoc inference, edit `nnunet_3d/simple_inference.py` (adjust `INPUT_IMAGE`, `OUTPUT_FOLDER`, `path/to/your/trained_model`, `device`, `use_folds`) and run `python nnunet_3d/simple_inference.py`.

### Evaluation metrics
Run the Python scripts in `src/metrics/` from the repo root; they create new CSVs/plots under `results/`:
- `basic_metrics.py` – Dice, HD95, confusion matrix, surface Dice, average surface distance.
- `cldice_metric.py` – centerline Dice.
- `boundary_F1_metric.py` – boundary continuity F1 between slices.
- `topo_error_metric.py` – topological violations, slice-wise error counts, critical-voxel maps.
- `fas_detect_metric.py` – fascicle-level IoU/Hausdorff pairs, detection sweeps, precision/recall, false negatives, split/merge counts.
- `fas_split_merge_metric.py` – graph-based fascicle tracking statistics across the stack.

### Figure and table generation
The R scripts in `src/plotting/` perform statistical analyses and generate figures/tables.

Run scripts from the repository root so `here()` resolves correctly, e.g.:

```bash
Rscript src/plotting/Fig3_seg_performance.R
Rscript src/plotting/TableS4_fascicle_area_analysis.R
```
